{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"자본분배율(IFRS)\",\"이윤분배율(IFRS)\" ,\"정상영업이익대비이자보상배율(IFRS)\" ], axis=1 , inplace=True)\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "print(df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_float = df.select_dtypes(exclude=\"object\")\n",
    "df_float = df[['거래소코드', '총자본증가율(IFRS)', '유형자산증가율(IFRS)', '비유동생물자산증가율(IFRS)',\n",
    "       '투자부동산증가율(IFRS)', '비유동자산증가율(IFRS)', '유동자산증가율(IFRS)', '재고자산증가율(IFRS)',\n",
    "       '자기자본증가율(IFRS)', '매출액증가율(IFRS)']]\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i, column in enumerate(df_float.columns):\n",
    "    plt.subplot(5,2,i+1)\n",
    "    plt.title(column)\n",
    "    plt.boxplot(df_float[column])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "for i in df_float.columns:\n",
    "    df_float[i] =  winsorize(df_float[i], limits=[0.05, 0.05])\n",
    "\n",
    "for k in df_float.columns:\n",
    "    df[i] = df_float[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = plt.cm.PuBu\n",
    "upp_mat = np.triu(df.select_dtypes(\"float\").corr(\"pearson\")) #  히트맵의 상삼각 행렬을 만들고, 이를 mask 매개변수로 전달하여 상삼각 행렬 부분을 숨김\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.rcParams['axes.unicode_minus'] = False # matplotlib에서 마이너스 기호 (-)를 제대로 표시하기 위한 설정\n",
    "plt.rc('font', family=\"Malgun Gothic\")\n",
    "plt.title(\"Correlation of Features\", y=1.05, size=15)\n",
    "sns.heatmap(df.select_dtypes(\"float\").corr(\"spearman\"), linewidths=0.1, vmax=1.0, vmin=-1.0, square=True,\n",
    "            cmap=colormap, linecolor=\"white\", annot=True, mask=upp_mat)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "vif[\"features\"] = df.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20230613\n",
    "\n",
    "for i in range(int(input())):\n",
    "    b, c = map(int,input().split())\n",
    "    print(b+c)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(int(input())//4):\n",
    "    i = \"long\"\n",
    "    print(i, end=\" \")\n",
    "    \n",
    "print(\"int\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif ,mutual_info_classif\n",
    "\n",
    "# f_classif: 분류 작업(classification tasks)에서 레이블(label)과 특징(feature) 사이의 ANOVA F-value를 계산하여 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# mutual_info_classif: 이산형(discrete) 타겟(target)을 위한 상호 정보(mutual information)를 계산하여 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# chi2: 분류 작업(classification tasks)에서 비음수 특징(features)의 카이제곱 통계량(chi-squared statistics)을 계산하여 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# f_regression: 회귀 작업(regression tasks)에서 레이블(label)과 특징(feature) 사이의 ANOVA F-value를 계산하여 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# mutual_info_regression: 연속형(continuous) 타겟(target)을 위한 상호 정보(mutual information)를 계산하여 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# SelectPercentile: 가장 높은 점수에 대한 백분율(percentile)을 기준으로 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# SelectFpr: 거짓 양성 비율(false positive rate) 테스트를 기준으로 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# SelectFdr: 추정된 거짓 발견 비율(false discovery rate)을 기준으로 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# SelectFwe: Family-wise error rate(FWE)를 기준으로 특징을 선택하는 기능을 제공합니다.\n",
    "\n",
    "# GenericUnivariateSelect: 설정 가능한 모드(mode)를 가진 단변량(univariate) 특징 선택기능을 제공합니다.\n",
    "\n",
    "\n",
    "# 아이리스 데이터 로드\n",
    "data = load_iris()\n",
    "X = data.data  # 독립 변수\n",
    "y = data.target  # 종속 변수\n",
    "\n",
    "# SelectKBest를 사용하여 변수 선택\n",
    "k = 2  # 선택할 변수 개수\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df.index = data.feature_names\n",
    "df[\"col\"] = selector.get_support()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression,ElasticNet\n",
    "\n",
    "# 아이리스 데이터 로드\n",
    "iris = load_iris()\n",
    "x_train = iris.data\n",
    "y_train = iris.target\n",
    "\n",
    "# 특성 선택을 저장할 데이터프레임 생성\n",
    "df_select = pd.DataFrame()\n",
    "df_select.index = data.feature_names\n",
    "\n",
    "# L1 규제를 적용한 특성 선택 수행\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l1', solver='liblinear', C=0.01)).fit(x_train, y_train)\n",
    "df_select[\"lasso_0.01\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l1', solver='liblinear', C=0.05)).fit(x_train, y_train)\n",
    "df_select[\"lasso_0.05\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l1', solver='liblinear', C=0.1)).fit(x_train, y_train)\n",
    "df_select[\"lasso_0.1\"] = selector.get_support()\n",
    "\n",
    "# L2 규제를 적용한 특성 선택 수행\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l2', solver='liblinear', C=0.01)).fit(x_train, y_train)\n",
    "df_select[\"ridge_0.01\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l2', solver='liblinear', C=0.05)).fit(x_train, y_train)\n",
    "df_select[\"ridge_0.05\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=LogisticRegression(penalty='l2', solver='liblinear', C=0.1)).fit(x_train, y_train)\n",
    "df_select[\"ridge_0.1\"] = selector.get_support()\n",
    "\n",
    "# ElasticNet 규제를 적용한 특성 선택 수행\n",
    "selector = SelectFromModel(estimator=ElasticNet(alpha=0.01, l1_ratio=0.5)).fit(x_train, y_train)\n",
    "df_select[\"elasticnet_0.01\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=ElasticNet(alpha=0.05, l1_ratio=0.5)).fit(x_train, y_train)\n",
    "df_select[\"elasticnet_0.05\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=ElasticNet(alpha=0.1, l1_ratio=0.5)).fit(x_train, y_train)\n",
    "df_select[\"elasticnet_0.1\"] = selector.get_support()\n",
    "\n",
    "# 랜덤 포레스트 분류기 모델 생성 및 훈련\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier(),threshold=0.1).fit(X, y)\n",
    "df_select[\"rf_0.1\"] = selector.get_support()\n",
    "\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier(),threshold=0.1).fit(X, y)\n",
    "df_select[\"rf_0.1\"] = selector.get_support()\n",
    "selector = SelectFromModel(estimator=RandomForestClassifier(),threshold=0.3).fit(X, y)\n",
    "df_select[\"rf_0.3\"] = selector.get_support()\n",
    "\n",
    "\n",
    "df_select[\"true_sum\"] = df_select.sum(axis=1)\n",
    "\n",
    "df_select\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(input())\n",
    "N_list = list(map(int, input().split()))\n",
    "v = int(input())\n",
    "print(N_list.count(v))\n",
    "\n",
    "\n",
    "\n",
    "a, b = map(int, input().split())\n",
    "c = list(map(int, input().split()))\n",
    "for i in range(N):\n",
    "    if c[i] < X:\n",
    "        print(a[i], end=\" \")\n",
    "\n",
    "ls = []\n",
    "\n",
    "for i in range(int(input())):\n",
    "    i = int(input())\n",
    "    ls.append(i)\n",
    "\n",
    "print(ls.max(), ls.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  정규성 검정하는 함수\n",
    "def normality_test(df,tool=\"Shapiro\"):\n",
    "    result_dict = {\"col\" :[], \"p_val\":[], \"t_val\":[]}\n",
    "    if tool == \"Shapiro\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = shapiro(df[i].values)\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"ks\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = kstest(df[i].values,\"norm\")\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"jarque\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = jarque_bera(df[i].values)\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"nomal\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = normaltest(df[i].values)\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"anderson\":\n",
    "        result_dict_1 = {\"col\" :[], \"critical_values\":[], \"t_val\":[]}\n",
    "        for i in df.columns:\n",
    "            result = anderson(df[i].values)\n",
    "            result_dict_1[\"col\"].append(i)\n",
    "            result_dict_1[\"critical_values\"].append(result.critical_values[2])\n",
    "            result_dict_1[\"t_val\"].append(result.statistic)\n",
    "            df_test = pd.DataFrame(result_dict_1) \n",
    "    return df_test\n",
    "\n",
    "# 등분산 검정 함수\n",
    "def homoscedasticity_test(df,tool):\n",
    "    result_dict = {\"col\":[], \"p_val\":[], \"t_val\":[]}\n",
    "    if tool == \"levene\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = levene(df[df[\"point\"]==1][i], df[df[\"point\"]==0][i])\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_homoscedasticity_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"bartlett\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = bartlett(df[df[\"point\"]==1][i], df[df[\"point\"]==0][i])\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_homoscedasticity_test = pd.DataFrame(result_dict)\n",
    "    return df_homoscedasticity_test\n",
    "\n",
    "# t 검정\n",
    "\n",
    "def t_test(df,tool):\n",
    "    result_dict = {\"col\":[], \"p_val\":[], \"t_val\":[]}\n",
    "    if tool == \"student\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = stats.ttest_ind(df[df['point']==1][i], df[df['point']==0][i])\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_t_test = pd.DataFrame(result_dict)\n",
    "    if tool == \"welch\":\n",
    "        for i in df.columns:\n",
    "            statistic, p_value = stats.ttest_ind(df[df['point']==1][i], df[df['point']==0][i],equal_var=False)\n",
    "            result_dict[\"col\"].append(i)\n",
    "            result_dict[\"p_val\"].append(p_value)\n",
    "            result_dict[\"t_val\"].append(statistic)\n",
    "            df_t_test = pd.DataFrame(result_dict)\n",
    "    return df_t_test\n",
    "\n",
    "\n",
    "\n",
    "def durbin(df):\n",
    "    dw_statistic = sm.stats.stattools.durbin_watson(df.target)\n",
    "    if dw_statistic < 2:\n",
    "        print(\"양의 자기상관이 있다.\",dw_statistic)\n",
    "    elif dw_statistic > 2:\n",
    "        print(\"음의 자기상관이 있다.\",dw_statistic)\n",
    "    else:\n",
    "        print(\"자기상관이 없다.\",dw_statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 변수의 다중공선성 확인 \n",
    "iris = load_iris()\n",
    "df =  pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# VIF 값과 각 Feature 이름에 대해 설정\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "vif[\"features\"] = df.columns \n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  변수 선택 후 결과 저장할 데이터 프레임 생성\n",
    "df_select = pd.DataFrame()\n",
    "df_select.index = x_train.columns\n",
    "\n",
    "# 트레인 테스트 데이터 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "selector = SequentialFeatureSelector(estimator=LinearRegression(), n_features_to_select=3, direction = \"forward\" , scoring='r2', cv=5,n_jobs=-1)\n",
    "selector.fit(x_train, y_train)\n",
    "df_select[\"fwd\"] = selector.get_support().tolist()\n",
    "\n",
    "selector = SequentialFeatureSelector(estimator=LinearRegression(), n_features_to_select=3, direction = \"backward\" , scoring='r2', cv=5,n_jobs=-1)\n",
    "selector.fit(x_train, y_train)\n",
    "df_select[\"bwd\"] = selector.get_support().tolist()\n",
    "\n",
    "df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_point = np.percentile(잔차[\"target\"], 50)\n",
    "잔차[\"point\"] = 0\n",
    "잔차.loc[잔차[\"target\"] > cut_off_point, \"point\"] = 1\n",
    "\n",
    "#  잔차 분석(정규성, 등분산성)\n",
    "\n",
    "print(normality_test(잔차))\n",
    "print(\"-----------------\")\n",
    "print(homoscedasticity_test(잔차,tool=\"levene\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
